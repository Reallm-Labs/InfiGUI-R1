<h1 align="center">
<img src="images/InfiGUI-R1_logo.png" width="100" alt="ToRA" />
<br>
InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners
</h1>

<br>
<div align="center" style="margin: 20px 0; display: flex; justify-content: center; gap: 15px;"> | 
<!--     <a href="https://b7277.github.io/InfiGUIAgent.github.io/" style="padding: 8px 15px; background-color: #2F4F4F; color: white; border-radius: 5px; text-decoration: none; transition: all 0.3s;">ğŸ  Homepage</a> |  -->
    <a href="https://arxiv.org/abs/2504.14239" style="padding: 8px 15px; background-color: #B22222; color: white; border-radius: 5px; text-decoration: none; transition: all 0.3s;">ğŸ“š Arxiv</a> | 
<!--     <a href="https://huggingface.co/papers/2501.04575" style="padding: 8px 15px; background-color: #FFD700; color: black; border-radius: 5px; text-decoration: none; transition: all 0.3s;">ğŸ¤— Paper</a> |  -->
    <a href="https://huggingface.co/Reallm-Labs/InfiGUI-R1-3B" style="padding: 8px 15px; background-color: #6495ED; color: white; border-radius: 5px; text-decoration: none; transition: all 0.3s;">ğŸ¤— Model</a> | 
</div>
<br>

This is the repo for the paper "[InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)". In this work, we develop **InfiGUI-R1**, a multimodal large language model-based GUI agent primarily trained using **Reinforcement Learning** to enhance planning and error recovery skills for GUI tasks. Specifically, our agent is trained using a two-stage framework: first, we inject spatial reasoning capabilities by distilling reasoning trajectories from teacher models; second, we enhance the agent's planning and error recovery skills using Reinforcement Learning, employing techniques like rewarding accurate sub-goal generation and training on constructed error recovery scenarios.

## ğŸ”¥  News
- ğŸ”¥[2025/4/21] Our paper "[InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)" released.
- ğŸ”¥[2025/1/9] Our paper "[InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection](https://arxiv.org/abs/2501.04575)" released.
- ğŸ”¥[2024/12/12] Our paper "[OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://os-agent-survey.github.io/)" released.
- [2024/4/2] Our paper "[InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks](https://infiagent.github.io/)" is accepted by *ICML 2024*.
